{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5899c787",
      "metadata": {
        "id": "5899c787"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations/blob/main/demos/increasing_in_distribution_combinations.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "529e5c54",
      "metadata": {
        "id": "529e5c54"
      },
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "This demo shows the impact of increasing in-distribution combinations on out-of-distribution generalization. Specifically, we reproduce the results for the MNIST Rotation dataset on the `SHARED` architecture.\n",
        "\n",
        "As shown below, this architecture enforces parameter sharing between the two tasks (category prediction and viewpoint prediction).\n",
        "\n",
        "As described in the paper, our results show that increasing data diversity (i.e. in-distribution combinations) leads to a substantial increase in out-of-distribution performance eventhough total number of training images (dataset size) is held constant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca791caa",
      "metadata": {
        "id": "ca791caa"
      },
      "source": [
        "![Shared Architecture](https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations/blob/main/docs/images/Shared.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-danish",
      "metadata": {
        "id": "dangerous-danish"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def create_folder(path):\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "766dc560",
      "metadata": {
        "id": "766dc560"
      },
      "source": [
        "If running on google colab, the below code does the following:\n",
        "- clone repo\n",
        "- set up necessary folders\n",
        "- download MNIST Rotation Dataset at appropriate place\n",
        "- unzip MNIST Rotation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a0fd8c",
      "metadata": {
        "id": "f9a0fd8c"
      },
      "source": [
        "#### If you're not running on colab, please follow download instructions to get the mnist_rotaiton dataset using:\n",
        "\n",
        "```\n",
        "cd utils\n",
        "bash download_mnist_rotation.sh\n",
        "```\n",
        "\n",
        "#### If not using google colab, please proceed below only after downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iraqi-singles",
      "metadata": {
        "id": "iraqi-singles"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Cloning code base to colab....')\n",
        "    !git clone https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations.git\n",
        "    !cd generalization_to_OOD_category_viewpoint_cominations/utils && bash download_mnist_rotation.sh\n",
        "    CODE_ROOT = \"generalization_to_OOD_category_viewpoint_cominations/\"\n",
        "else:\n",
        "    CODE_ROOT = '..'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tired-assessment",
      "metadata": {
        "id": "tired-assessment"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "from PIL import ImageFile\n",
        "import random\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import argparse\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('%s/res/'%CODE_ROOT)\n",
        "from models.models import get_model\n",
        "from loader.loader import get_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clinical-device",
      "metadata": {
        "id": "clinical-device"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"poster\")\n",
        "sns.set_palette(\"Set1\", 8, .75)\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4439267",
      "metadata": {
        "id": "b4439267"
      },
      "source": [
        "This demo trains networks with 1, 3, 6 and 9 in-distribution combinations of the MNIST-Rotation dataset, and plots performance on out-of-distribution combinations from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e28193b",
      "metadata": {
        "id": "1e28193b"
      },
      "source": [
        "To run on a different architecture, please change the `ARCH` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indoor-warrior",
      "metadata": {
        "id": "indoor-warrior"
      },
      "outputs": [],
      "source": [
        "DATASET_NAMES = ['mnist_rotation_one_by_nine', 'mnist_rotation_three_by_nine',\n",
        "                 'mnist_rotation_six_by_nine', 'mnist_rotation_nine_by_nine']\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 100\n",
        "ARCHS =['EARLY_BRANCHING_COMBINED','SPLIT_AFTER_ONE_BLOCK', 'LATE_BRANCHING_COMBINED', 'LATE_BRANCHING_COMBINED_WIDER',  'LATE_BRANCHING_COMBINED_ONE_FOURTH', 'MULTITASK_INCEPTION_WIDE', 'MULTITASK_RESNEXT_WIDE',  'MULTITASK_RESNEXT', 'LATE_BRANCHING_COMBINED_HALF','Multitask_Resnet_Early_New',  'SPLIT_AFTER_THREE_BLOCKS', 'Two_Block_Encoder_Long_Decoder']\n",
        "\n",
        "image_transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "\n",
        "GPU = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PSSncai30Viw"
      },
      "id": "PSSncai30Viw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "soviet-prague",
      "metadata": {
        "id": "soviet-prague",
        "tags": []
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = (10,10,10,10)\n",
        "loader_new = get_loader('multi_attribute_loader_file_list_mnist_rotation')\n",
        "\n",
        "file_list_root = '%s/dataset_lists/mnist_rotation_lists/'%CODE_ROOT\n",
        "att_path = '%s/dataset_lists/combined_attributes.p'%CODE_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "crucial-diameter",
      "metadata": {
        "id": "crucial-diameter"
      },
      "outputs": [],
      "source": [
        "shuffles = {'train':True,'val':True,'test':False}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "million-joseph",
      "metadata": {
        "id": "million-joseph"
      },
      "outputs": [],
      "source": [
        "data_dir = '%s/data/'%CODE_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "documentary-builder",
      "metadata": {
        "id": "documentary-builder"
      },
      "outputs": [],
      "source": [
        "all_dsets = {}\n",
        "all_dset_loaders = {}\n",
        "all_dset_sizes = {}\n",
        "\n",
        "for DATASET_NAME in DATASET_NAMES:\n",
        "    file_lists = {}\n",
        "    dsets = {}\n",
        "    dset_loaders = {}\n",
        "    dset_sizes = {}\n",
        "    for phase in ['train','val','test']:\n",
        "        file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
        "        dsets[phase] = loader_new(file_lists[phase],att_path, image_transform, data_dir)\n",
        "        dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
        "        dset_sizes[phase] = len(dsets[phase])\n",
        "    all_dsets[DATASET_NAME] = dsets\n",
        "    all_dset_loaders[DATASET_NAME] = dset_loaders\n",
        "    all_dset_sizes[DATASET_NAME] = dset_sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "partial-edinburgh",
      "metadata": {
        "id": "partial-edinburgh"
      },
      "outputs": [],
      "source": [
        "multi_losses = [nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, num_class=3, training=True):\n",
        "        super(Inception, self).__init__()\n",
        "        model = models.inception_v3(pretrained=True)\n",
        "        self.inception_conv1 = model.Conv2d_1a_3x3\n",
        "        self.inception_conv2 = model.Conv2d_2a_3x3\n",
        "        self.inception_conv3 = model.Conv2d_2b_3x3\n",
        "        self.maxpool1 = model.maxpool1\n",
        "        self.inception_conv4 = model.Conv2d_3b_1x1\n",
        "        self.inception_conv5 = model.Conv2d_4a_3x3\n",
        "        self.maxpool2 = model.maxpool2\n",
        "        self.mixed1 = model.Mixed_5b\n",
        "        self.mixed2 = model.Mixed_5c\n",
        "        self.mixed3 = model.Mixed_5d\n",
        "        self.mixed4 = model.Mixed_6a\n",
        "        self.mixed5 = model.Mixed_6b\n",
        "        self.mixed6 = model.Mixed_6c\n",
        "        self.mixed7 = model.Mixed_6d\n",
        "        self.mixed8 = model.Mixed_6e\n",
        "        if training:\n",
        "            self.auxlogits = model.AuxLogits.conv0\n",
        "            self.auxlogits1 = model.AuxLogits.conv1\n",
        "            self.auxlogits2 = nn.Linear(768, num_class)\n",
        "        self.mixed9 = model.Mixed_7a\n",
        "        self.mixed10 = model.Mixed_7b\n",
        "        self.mixed11 = model.Mixed_7c\n",
        "        self.avgpool = model.avgpool\n",
        "        self.fc = nn.Linear(2048, 2048)\n",
        "        self.bnlast = nn.BatchNorm1d(2048)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, num_class)\n",
        "\n",
        "        self.training = training\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.size(1) == 3\n",
        "        x = self.inception_conv1(x)\n",
        "        x = self.inception_conv2(x)\n",
        "        x = self.inception_conv3(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.inception_conv4(x)\n",
        "        x = self.inception_conv5(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.mixed1(x)\n",
        "        x = self.mixed2(x)\n",
        "        x = self.mixed3(x)\n",
        "        x = self.mixed4(x)\n",
        "        x = self.mixed5(x)\n",
        "        x = self.mixed6(x)\n",
        "        x = self.mixed7(x)\n",
        "        x = self.mixed8(x)\n",
        "\n",
        "        if self.training:\n",
        "            aux = self.auxlogits(x)\n",
        "            aux = self.auxlogits1(aux)\n",
        "            aux = aux.view(aux.size(0), -1)\n",
        "            aux = self.auxlogits2(aux)\n",
        "        x = self.mixed9(x)\n",
        "        x = self.mixed10(x)\n",
        "        x = self.mixed11(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.classifier(x)\n",
        "        if self.training:\n",
        "            return x, aux\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, c_in):\n",
        "        super().__init__()\n",
        "        self.globalavgpooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(c_in, max(1, c_in // 16))\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(max(1, c_in // 16), c_in)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.c_in = c_in\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert self.c_in == x.size(1)\n",
        "        x = self.globalavgpooling(x)\n",
        "        x = x.squeeze()\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomDownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, intermediate_channels, out_channels):\n",
        "        super(CustomDownsampleBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, intermediate_channels, 1)\n",
        "        self.conv2 = nn.Conv2d(intermediate_channels, intermediate_channels, 3, 2, 1)\n",
        "        self.conv3 = nn.Conv2d(intermediate_channels, out_channels, 1)\n",
        "        self.avgpool = nn.AvgPool2d(2, 2, ceil_mode=True)\n",
        "        self.conv4 = nn.Conv2d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        branch = self.avgpool(branch)\n",
        "        branch = self.conv4(branch)\n",
        "        return x + branch\n",
        "\n",
        "\n",
        "class CustomResNet50(nn.Module):\n",
        "    def __init__(self, num_class=3, intermediate_channels=[64, 128, 256, 512]):\n",
        "        super(CustomResNet50, self).__init__()\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.layer1 = model.layer1\n",
        "        model.layer1[0] = CustomDownsampleBlock(64, intermediate_channels[0], 256)\n",
        "\n",
        "        self.layer2 = model.layer2\n",
        "        model.layer2[0] = CustomDownsampleBlock(256, intermediate_channels[1], 512)\n",
        "\n",
        "        self.layer3 = model.layer3\n",
        "        model.layer3[0] = CustomDownsampleBlock(512, intermediate_channels[2], 1024)\n",
        "\n",
        "        self.layer4 = model.layer4\n",
        "        model.layer4[0] = CustomDownsampleBlock(1024, intermediate_channels[3], 2048)\n",
        "\n",
        "        self.avgpool = model.avgpool\n",
        "\n",
        "        self.fc = nn.Linear(2048, 2048)\n",
        "        self.bnlast = nn.BatchNorm1d(2048)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "    def __init__(self, num_class=3, isCustom=False, backend=\"resnet50\"):\n",
        "        super(Baseline, self).__init__()\n",
        "        model = getattr(models, backend)(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.layer1 = model.layer1\n",
        "        self.layer2 = model.layer2\n",
        "        self.layer3 = model.layer3\n",
        "        self.layer4 = model.layer4\n",
        "        self.avgpool = model.avgpool\n",
        "\n",
        "        self.fc = nn.Linear(2048, 2048)\n",
        "        self.bnlast = nn.BatchNorm1d(2048)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, num_class)\n",
        "\n",
        "        self.isCustom = isCustom\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.classifier(x)\n",
        "        if self.isCustom:\n",
        "            x = torch.sigmoid(x).squeeze()\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet50DC5(nn.Module):\n",
        "    def __init__(self, dilation=True, num_class=3):\n",
        "        super(ResNet50DC5, self).__init__()\n",
        "        model = models.resnet50(pretrained=True,\n",
        "                                replace_stride_with_dilation=[False, False, dilation])\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.layer1 = model.layer1\n",
        "        self.layer2 = model.layer2\n",
        "        self.layer3 = model.layer3\n",
        "        self.layer4 = model.layer4\n",
        "        self.avgpool = model.avgpool\n",
        "\n",
        "        self.fc = nn.Linear(2048, 2048)\n",
        "        self.bnlast = nn.BatchNorm1d(2048)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, num_class=3):\n",
        "        super(ResNet50, self).__init__()\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.layer1 = model.layer1\n",
        "        for i in range(len(self.layer1)):\n",
        "            for name, module in self.layer1[i].named_modules():\n",
        "                if \"bn3\" in name:\n",
        "                    nn.init.constant_(module.weight, 0.)\n",
        "        self.layer2 = model.layer2\n",
        "        for i in range(len(self.layer2)):\n",
        "            for name, module in self.layer2[i].named_modules():\n",
        "                if \"bn3\" in name:\n",
        "                    nn.init.constant_(module.weight, 0.)\n",
        "        self.layer3 = model.layer3\n",
        "        for i in range(len(self.layer3)):\n",
        "            for name, module in self.layer3[i].named_modules():\n",
        "                if \"bn3\" in name:\n",
        "                    nn.init.constant_(module.weight, 0.)\n",
        "        self.layer4 = model.layer4\n",
        "        for i in range(len(self.layer4)):\n",
        "            for name, module in self.layer4[i].named_modules():\n",
        "                if \"bn3\" in name:\n",
        "                    nn.init.constant_(module.weight, 0.)\n",
        "        self.avgpool = model.avgpool\n",
        "\n",
        "        self.fc = nn.Linear(2048, 2048)\n",
        "        self.bnlast = nn.BatchNorm1d(2048)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.fc1 = nn.Linear(2048, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.classifier = nn.Linear(256, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SEDense34(nn.Module):\n",
        "    def __init__(self, num_class=3, needs_norm=True):\n",
        "        super().__init__()\n",
        "        model = models.resnet34(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        # layer1\n",
        "        self.bottleneck11 = model.layer1[0]\n",
        "        self.bottleneck12 = model.layer1[1]\n",
        "        self.bottleneck13 = model.layer1[2]\n",
        "\n",
        "        self.seblock11 = SEBlock(64)\n",
        "        self.seblock12 = SEBlock(64)\n",
        "        self.seblock13 = SEBlock(64)\n",
        "        # layer2\n",
        "        self.bottleneck21 = model.layer2[0]\n",
        "        self.bottleneck22 = model.layer2[1]\n",
        "        self.bottleneck23 = model.layer2[2]\n",
        "        self.bottleneck24 = model.layer2[3]\n",
        "\n",
        "        self.auxconv1 = nn.Conv2d(64, 128, 1, 2, 0)\n",
        "        self.optionalbn1 = nn.BatchNorm2d(128)\n",
        "        self.seblock21 = SEBlock(128)\n",
        "        self.seblock22 = SEBlock(128)\n",
        "        self.seblock23 = SEBlock(128)\n",
        "        self.seblock24 = SEBlock(128)\n",
        "        # layer3\n",
        "        self.bottleneck31 = model.layer3[0]\n",
        "        self.bottleneck32 = model.layer3[1]\n",
        "        self.bottleneck33 = model.layer3[2]\n",
        "        self.bottleneck34 = model.layer3[3]\n",
        "        self.bottleneck35 = model.layer3[4]\n",
        "        self.bottleneck36 = model.layer3[5]\n",
        "\n",
        "        self.auxconv2 = nn.Conv2d(128, 256, 1, 2, 0)\n",
        "        self.optionalbn2 = nn.BatchNorm2d(256)\n",
        "        self.seblock31 = SEBlock(256)\n",
        "        self.seblock32 = SEBlock(256)\n",
        "        self.seblock33 = SEBlock(256)\n",
        "        self.seblock34 = SEBlock(256)\n",
        "        self.seblock35 = SEBlock(256)\n",
        "        self.seblock36 = SEBlock(256)\n",
        "        # layer4\n",
        "        self.bottleneck41 = model.layer4[0]\n",
        "        self.bottleneck42 = model.layer4[1]\n",
        "        self.bottleneck43 = model.layer4[2]\n",
        "\n",
        "        self.auxconv3 = nn.Conv2d(256, 512, 1, 2, 0)\n",
        "        self.optionalbn3 = nn.BatchNorm2d(512)\n",
        "        self.seblock41 = SEBlock(512)\n",
        "        self.seblock42 = SEBlock(512)\n",
        "        self.seblock43 = SEBlock(512)\n",
        "\n",
        "        self.avgpool = model.avgpool\n",
        "        self.fc = nn.Linear(512, 128)\n",
        "        self.bnlast = nn.BatchNorm1d(128)\n",
        "        self.relulast = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "        self.classifier = nn.Linear(128, num_class)\n",
        "\n",
        "        self.norm = needs_norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "\n",
        "        branch1 = x\n",
        "        x = self.bottleneck11(x)\n",
        "        scale1 = self.seblock11(x)\n",
        "        x = scale1 * x + branch1\n",
        "\n",
        "        branch2 = x\n",
        "        x = self.bottleneck12(x)\n",
        "        scale2 = self.seblock12(x)\n",
        "        x = scale2 * x + branch2\n",
        "\n",
        "        branch3 = x\n",
        "        x = self.bottleneck13(x)\n",
        "        scale3 = self.seblock13(x)\n",
        "        x = scale3 * x + branch3\n",
        "\n",
        "        branch4 = x\n",
        "        x = self.bottleneck21(x)\n",
        "        scale4 = self.seblock21(x)\n",
        "        if self.norm:\n",
        "            x = scale4 * x + self.optionalbn1(self.auxconv1(branch4))\n",
        "        else:\n",
        "            x = scale4 * x + self.auxconv1(branch4)\n",
        "\n",
        "        branch5 = x\n",
        "        x = self.bottleneck22(x)\n",
        "        scale5 = self.seblock22(x)\n",
        "        x = scale5 * x + branch5\n",
        "\n",
        "        branch6 = x\n",
        "        x = self.bottleneck23(x)\n",
        "        scale6 = self.seblock23(x)\n",
        "        x = scale6 * x + branch6\n",
        "\n",
        "        branch7 = x\n",
        "        x = self.bottleneck24(x)\n",
        "        scale7 = self.seblock24(x)\n",
        "        x = scale7 * x + branch7\n",
        "\n",
        "        branch8 = x\n",
        "        x = self.bottleneck31(x)\n",
        "        scale8 = self.seblock31(x)\n",
        "        if self.norm:\n",
        "            x = scale8 * x + self.optionalbn2(self.auxconv2(branch8))\n",
        "        else:\n",
        "            x = scale8 * x + self.auxconv2(branch8)\n",
        "\n",
        "        branch9 = x\n",
        "        x = self.bottleneck32(x)\n",
        "        scale9 = self.seblock32(x)\n",
        "        x = scale9 * x + branch9\n",
        "\n",
        "        branch10 = x\n",
        "        x = self.bottleneck33(x)\n",
        "        scale10 = self.seblock33(x)\n",
        "        x = scale10 * x + branch10\n",
        "\n",
        "        branch11 = x\n",
        "        x = self.bottleneck34(x)\n",
        "        scale11 = self.seblock34(x)\n",
        "        x = scale11 * x + branch11\n",
        "\n",
        "        branch12 = x\n",
        "        x = self.bottleneck35(x)\n",
        "        scale12 = self.seblock35(x)\n",
        "        x = scale12 * x + branch12\n",
        "\n",
        "        branch13 = x\n",
        "        x = self.bottleneck36(x)\n",
        "        scale13 = self.seblock36(x)\n",
        "        x = scale13 * x + branch13\n",
        "\n",
        "        branch14 = x\n",
        "        x = self.bottleneck41(x)\n",
        "        scale14 = self.seblock41(x)\n",
        "        if self.norm:\n",
        "            x = scale14 * x + self.optionalbn3(self.auxconv3(branch14))\n",
        "        else:\n",
        "            x = scale14 * x + self.auxconv3(branch14)\n",
        "\n",
        "        branch15 = x\n",
        "        x = self.bottleneck42(x)\n",
        "        scale15 = self.seblock42(x)\n",
        "        x = scale15 * x + branch15\n",
        "\n",
        "        branch16 = x\n",
        "        x = self.bottleneck43(x)\n",
        "        scale16 = self.seblock43(x)\n",
        "        x = scale16 * x + branch16\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bnlast(x)\n",
        "        x = self.relulast(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SEDense18(nn.Module):\n",
        "    def __init__(self, num_class=3, needs_norm=True):\n",
        "        super().__init__()\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        self.conv0 = model.conv1\n",
        "        self.bn0 = model.bn1\n",
        "        self.relu0 = model.relu\n",
        "        self.pooling0 = model.maxpool\n",
        "        self.basicBlock11 = model.layer1[0]\n",
        "        self.seblock1 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock12 = model.layer1[1]\n",
        "        self.seblock2 = SEBlock(64)\n",
        "\n",
        "        self.basicBlock21 = model.layer2[0]\n",
        "        self.seblock3 = SEBlock(128)\n",
        "        self.ancillaryconv3 = nn.Conv2d(64, 128, 1, 2, 0)\n",
        "        self.optionalNorm2dconv3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.basicBlock22 = model.layer2[1]\n",
        "        self.seblock4 = SEBlock(128)\n",
        "\n",
        "        self.basicBlock31 = model.layer3[0]\n",
        "        self.seblock5 = SEBlock(256)\n",
        "        self.ancillaryconv5 = nn.Conv2d(128, 256, 1, 2, 0)\n",
        "        self.optionalNorm2dconv5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.basicBlock32 = model.layer3[1]\n",
        "        self.seblock6 = SEBlock(256)\n",
        "\n",
        "        self.basicBlock41 = model.layer4[0]\n",
        "        # last stride = 1\n",
        "        self.basicBlock41.conv1 = nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False,\n",
        "                                            device=\"cuda:0\")\n",
        "        self.basicBlock41.downsample[0] = nn.Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False,\n",
        "                                                    device=\"cuda:0\")\n",
        "        self.seblock7 = SEBlock(512)\n",
        "        self.ancillaryconv7 = nn.Conv2d(256, 512, 1, 1, 0)\n",
        "        self.optionalNorm2dconv7 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.basicBlock42 = model.layer4[1]\n",
        "        self.seblock8 = SEBlock(512)\n",
        "\n",
        "        self.avgpooling = model.avgpool\n",
        "        # self.fc = nn.Linear(512, num_class)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, num_class),\n",
        "        )\n",
        "        self.needs_norm = needs_norm\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.bn0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.pooling0(x)\n",
        "        branch1 = x\n",
        "        x = self.basicBlock11(x)\n",
        "        scale1 = self.seblock1(x)\n",
        "        x = scale1 * x + branch1\n",
        "\n",
        "        branch2 = x\n",
        "        x = self.basicBlock12(x)\n",
        "        scale2 = self.seblock2(x)\n",
        "        x = scale2 * x + branch2\n",
        "\n",
        "        branch3 = x\n",
        "        x = self.basicBlock21(x)\n",
        "        scale3 = self.seblock3(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale3 * x + self.optionalNorm2dconv3(self.ancillaryconv3(branch3))\n",
        "        else:\n",
        "            x = scale3 * x + self.ancillaryconv3(branch3)\n",
        "\n",
        "        branch4 = x\n",
        "        x = self.basicBlock22(x)\n",
        "        scale4 = self.seblock4(x)\n",
        "        x = scale4 * x + branch4\n",
        "\n",
        "        branch5 = x\n",
        "        x = self.basicBlock31(x)\n",
        "        scale5 = self.seblock5(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale5 * x + self.optionalNorm2dconv5(self.ancillaryconv5(branch5))\n",
        "        else:\n",
        "            x = scale5 * x + self.ancillaryconv5(branch5)\n",
        "\n",
        "        branch6 = x\n",
        "        x = self.basicBlock32(x)\n",
        "        scale6 = self.seblock6(x)\n",
        "        x = scale6 * x + branch6\n",
        "\n",
        "        branch7 = x\n",
        "        x = self.basicBlock41(x)\n",
        "        scale7 = self.seblock7(x)\n",
        "        if self.needs_norm:\n",
        "            x = scale7 * x + self.optionalNorm2dconv7(self.ancillaryconv7(branch7))\n",
        "        else:\n",
        "            x = scale7 * x + self.ancillaryconv7(branch7)\n",
        "\n",
        "        branch8 = x\n",
        "        x = self.basicBlock42(x)\n",
        "        scale8 = self.seblock8(x)\n",
        "        x = scale8 * x + branch8\n",
        "\n",
        "        x = self.avgpooling(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wMcUJKMRGusp"
      },
      "id": "wMcUJKMRGusp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entertaining-interface",
      "metadata": {
        "id": "entertaining-interface"
      },
      "outputs": [],
      "source": [
        "def weight_scheduler(epoch_num, task):\n",
        "    if task == 'shared':\n",
        "        return [0.0,1.0,0.0,1.0]\n",
        "    elif task == 'viewpoint':\n",
        "        return [0.0,1.0,0.0,0.0]\n",
        "    elif task == 'category':\n",
        "        return [0.0,0.0,0.0,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dress-reminder",
      "metadata": {
        "id": "dress-reminder"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dset_loaders, dset_sizes, model, task, optimizer):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    phase = 'train'\n",
        "    \n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in dset_loaders[phase]:\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "        \n",
        "        phase_epoch_loss += calculated_loss\n",
        "        calculated_loss.backward()\n",
        "        optimizer.step()\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Train loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "\n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return model, epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vanilla-wisconsin",
      "metadata": {
        "id": "vanilla-wisconsin"
      },
      "outputs": [],
      "source": [
        "def test_epoch(dset_loaders, dset_sizes, model, best_model, best_test_loss, best_test_gm, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'val'\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in dset_loaders[phase]:\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Test loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    if epoch_loss < best_test_loss:\n",
        "        best_model = model\n",
        "        best_test_loss = epoch_loss\n",
        "        best_test_gm = epoch_gm\n",
        "    \n",
        "    return best_model, epoch_loss, epoch_gm, best_test_loss, best_test_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circular-phase",
      "metadata": {
        "id": "circular-phase"
      },
      "outputs": [],
      "source": [
        "def unseen_test_epoch(dset_loaders, dset_sizes, model, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'test'\n",
        "\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in dset_loaders[phase]:\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lesser-disaster",
      "metadata": {
        "id": "lesser-disaster"
      },
      "outputs": [],
      "source": [
        "plt.rc('xtick', labelsize=14) \n",
        "plt.rc('ytick', labelsize=14) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "senior-progress",
      "metadata": {
        "scrolled": true,
        "id": "senior-progress"
      },
      "outputs": [],
      "source": [
        "DATASET_NAMES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dynamic-output",
      "metadata": {
        "id": "dynamic-output"
      },
      "outputs": [],
      "source": [
        "dataset_titles = {}\n",
        "dataset_titles['mnist_rotation_one_by_nine'] = \"10% combinations seen\"\n",
        "dataset_titles['mnist_rotation_three_by_nine'] = \"30% combinations seen\"\n",
        "dataset_titles['mnist_rotation_six_by_nine'] = \"60% combinations seen\"\n",
        "dataset_titles['mnist_rotation_nine_by_nine'] = \"90% combinations seen\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modified-listening",
      "metadata": {
        "id": "modified-listening",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for ARCH in ARCHS:\n",
        "  all_train_all = {}\n",
        "  all_train_all[ARCH + ' ' + 'shared'] = [0]\n",
        "  all_train_all[ARCH + ' ' +'separate'] = [0]\n",
        "\n",
        "  all_test_all = {}\n",
        "  all_test_all[ARCH + ' ' +'shared'] = [0]\n",
        "  all_test_all[ARCH + ' ' +'separate'] = [0]\n",
        "\n",
        "  all_unseen_test_all = {}\n",
        "  all_unseen_test_all[ARCH + ' ' +'shared'] = [0]\n",
        "  all_unseen_test_all[ARCH + ' ' + 'separate'] = [0]\n",
        "\n",
        "  for DATASET_NAME in DATASET_NAMES:\n",
        "      print('______________________________________________________')\n",
        "      print('model_name:',ARCH,'  comb_name', dataset_titles[DATASET_NAME])\n",
        "      dsets = all_dsets[DATASET_NAME]\n",
        "      dset_loaders = all_dset_loaders[DATASET_NAME]\n",
        "      dset_sizes = all_dset_sizes[DATASET_NAME]\n",
        "      \n",
        "      models = {}\n",
        "\n",
        "      models['shared']= get_model(ARCH,NUM_CLASSES)\n",
        "      models['viewpoint']= get_model(ARCH,NUM_CLASSES)\n",
        "      models['category']= get_model(ARCH,NUM_CLASSES)\n",
        "\n",
        "      models['shared'].cuda();\n",
        "      models['viewpoint'].cuda();\n",
        "      models['category'].cuda();\n",
        "\n",
        "      best_models = {}\n",
        "      best_models['shared'] = models['shared']\n",
        "      best_models['viewpoint'] = models['viewpoint']\n",
        "      best_models['category'] = models['category']\n",
        "\n",
        "      best_test_loss = 100\n",
        "      best_test_gm = 0\n",
        "\n",
        "      all_train_gms = {}\n",
        "      all_train_gms['shared'] = [0]\n",
        "      all_train_gms['separate'] = [0]\n",
        "\n",
        "      all_test_gms = {}\n",
        "      all_test_gms['shared'] = [0]\n",
        "      all_test_gms['separate'] = [0]\n",
        "\n",
        "      all_unseen_test_gms = {}\n",
        "      all_unseen_test_gms['shared'] = [0]\n",
        "      all_unseen_test_gms['separate'] = [0]\n",
        "\n",
        "      optimizers = {}\n",
        "      optimizers['shared'] = optim.Adam(models['shared'].parameters(), lr=0.001)\n",
        "      optimizers['viewpoint'] = optim.Adam(models['viewpoint'].parameters(), lr=0.001)\n",
        "      optimizers['category'] = optim.Adam(models['category'].parameters(), lr=0.001)\n",
        "      for epoch in range(NUM_EPOCHS):\n",
        "          train_gm_separate = 1\n",
        "          test_gm_separate = 1\n",
        "          unseen_test_gm_separate = 1\n",
        "\n",
        "          for TASK in ['viewpoint','category','shared']:\n",
        "              print('Epoch: %s, Task: %s'%(epoch,TASK))\n",
        "              print('---------')\n",
        "              models[TASK], train_loss, train_gm = train_epoch(dset_loaders, dset_sizes, models[TASK], TASK, optimizers[TASK])\n",
        "              best_models[TASK], test_loss, test_gm, best_test_loss, best_test_gm = test_epoch(dset_loaders, dset_sizes, models[TASK], best_models[TASK], best_test_loss, best_test_gm, TASK)\n",
        "              unseen_test_loss, unseen_test_gm = unseen_test_epoch(dset_loaders, dset_sizes, models[TASK], TASK)\n",
        "\n",
        "              if TASK != 'shared':\n",
        "                  train_gm_separate = train_gm_separate * train_gm\n",
        "                  test_gm_separate = test_gm_separate * test_gm\n",
        "                  unseen_test_gm_separate = unseen_test_gm_separate * test_gm\n",
        "\n",
        "          all_train_gms['separate'].append(np.sqrt(train_gm_separate))\n",
        "          all_test_gms['separate'].append(np.sqrt(test_gm_separate))\n",
        "          all_unseen_test_gms['separate'].append(np.sqrt(unseen_test_gm_separate))\n",
        "          all_train_gms['shared'].append(train_gm)\n",
        "          all_test_gms['shared'].append(test_gm)\n",
        "          all_unseen_test_gms['shared'].append(np.sqrt(unseen_test_gm))\n",
        "\n",
        "\n",
        "\n",
        "      all_train_all[ARCH + ' ' +'separate'].append(np.sqrt(train_gm_separate))\n",
        "      all_test_all[ARCH + ' ' +'separate'].append(np.sqrt(test_gm_separate))\n",
        "      all_unseen_test_all[ARCH + ' ' +'separate'].append(np.sqrt(unseen_test_gm_separate))\n",
        "      all_train_all[ARCH + ' ' +'shared'].append(train_gm)\n",
        "      all_test_all[ARCH + ' ' +'shared'].append(test_gm)\n",
        "      all_unseen_test_all[ARCH + ' ' +'shared'].append(np.sqrt(unseen_test_gm))\n",
        "      \n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      # fig,ax = plt.subplots(1, 3, figsize=(18,6))\n",
        "      # fig.suptitle(dataset_titles[DATASET_NAME], fontsize = 30)\n",
        "      # l1 = ax[0].plot(all_train_gms['separate'], color = 'blue', marker = 'o', markersize=5)[0]\n",
        "      # l2 = ax[0].plot(all_train_gms['shared'], color = 'red', marker = 'o', markersize=5)[0]\n",
        "      # ax[0].set_title('Train Accuracy', fontsize=12)\n",
        "      # line_labels = [\"Separate\", \"Shared\"]\n",
        "\n",
        "      # ax[1].plot(all_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "      # ax[1].plot(all_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "      # ax[1].set_title('Test Accuracy on Seen \\n Category-Viewpoint Combinations', fontsize=12)\n",
        "\n",
        "      # ax[2].plot(all_unseen_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "      # ax[2].plot(all_unseen_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "      # ax[2].set_title('Test Accuracy on Unseen \\n Category-Viewpoint Combinations', fontsize=12)\n",
        "      # fig.legend([l1, l2],     # The line objects\n",
        "      #         labels=line_labels,   # The labels for each line\n",
        "      #         loc=\"center right\",   # Position of legend\n",
        "      #         borderaxespad=0.2,    # Small spacing around legend box\n",
        "      #         prop={\"size\":20})\n",
        "      # plt.subplots_adjust(right=0.85, top =0.80)\n",
        "      # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CQf4xtuRGbD4"
      },
      "id": "CQf4xtuRGbD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = [0.1, 0.3, 0.6, 0.9]"
      ],
      "metadata": {
        "id": "z5z9iuekZByT"
      },
      "id": "z5z9iuekZByT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,8))\n",
        "label = []\n",
        "for key, value in all_train_all.items():\n",
        "    sns.lineplot(y = value, x =  percentage)\n",
        "    label.append(key)\n",
        "plt.legend(label)\n",
        "plt.xlabel('percentage of combination')\n",
        "plt.ylabel('acc')\n",
        "plt.title('train acc of different CNN models performance om Out-Of-Distribution Category-Viewpoint Generalization.')"
      ],
      "metadata": {
        "id": "6H3qB1gfoauM"
      },
      "id": "6H3qB1gfoauM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462bfeb3",
      "metadata": {
        "id": "462bfeb3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,8))\n",
        "label = []\n",
        "for key, value in all_test_all.items():\n",
        "    sns.lineplot(y = value, x =  percentage)\n",
        "    label.append(key)\n",
        "plt.legend(label)\n",
        "plt.xlabel('percentage of combination')\n",
        "plt.ylabel('acc')\n",
        "plt.title('test acc of different CNN models performance om Out-Of-Distribution Category-Viewpoint Generalization.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,8))\n",
        "label = []\n",
        "for key, value in all_unseen_test_all.items():\n",
        "    sns.lineplot(y = value, x =  percentage)\n",
        "    label.append(key)\n",
        "plt.legend(label)\n",
        "plt.xlabel('percentage of combination')\n",
        "plt.ylabel('acc')\n",
        "plt.title('unseen_test acc of different CNN models performance om Out-Of-Distribution Category-Viewpoint Generalization.')"
      ],
      "metadata": {
        "id": "fMU7cd5vF35b"
      },
      "id": "fMU7cd5vF35b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "milestone1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}